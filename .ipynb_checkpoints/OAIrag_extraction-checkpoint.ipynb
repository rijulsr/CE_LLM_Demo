{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b98b5b-0a51-424e-8369-f3a774e50cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in /home/rijul/miniconda3/lib/python3.13/site-packages (1.26.4)\n",
      "Requirement already satisfied: pillow in /home/rijul/miniconda3/lib/python3.13/site-packages (11.3.0)\n",
      "Requirement already satisfied: openai in /home/rijul/miniconda3/lib/python3.13/site-packages (1.106.1)\n",
      "Requirement already satisfied: pandas in /home/rijul/miniconda3/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: openpyxl in /home/rijul/miniconda3/lib/python3.13/site-packages (3.1.5)\n",
      "Requirement already satisfied: langchain in /home/rijul/miniconda3/lib/python3.13/site-packages (0.3.27)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: faiss-cpu in /home/rijul/miniconda3/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /home/rijul/miniconda3/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/rijul/miniconda3/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/rijul/miniconda3/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/rijul/miniconda3/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /home/rijul/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/rijul/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/rijul/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/rijul/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rijul/miniconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/rijul/miniconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/rijul/miniconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/rijul/miniconda3/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain) (0.4.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/rijul/miniconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/rijul/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rijul/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/rijul/miniconda3/lib/python3.13/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.11.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/rijul/miniconda3/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/rijul/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/rijul/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
      "Downloading tiktoken-0.11.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [langchain-openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed langchain-openai-0.3.32 tiktoken-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF pillow openai pandas openpyxl langchain langchain-openai faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33cfb0fa-ac2d-421a-a1d0-0716624430d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, time, base64, json, glob\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from pydantic import BaseModel, field_validator\n",
    "from typing import Optional, Union\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not set. Please set it before proceeding.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4d5a389-39fe-4993-9fb9-72b139f14e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "CARDS_DIRS = [\n",
    "    \"/home/rijul/Gitlaboratory/Context_Engineering_LLM/cards/abbr\",    # dermatology_core.v1.json\n",
    "    \"/home/rijul/Gitlaboratory/Context_Engineering_LLM/cards/lexicon\",    # meds_observed.v1.json\n",
    "    \"/home/rijul/Gitlaboratory/Context_Engineering_LLM/cards/policy\",     # notation.v1.json, units.v1.json, date.v1.json\n",
    "    \"/home/rijul/Gitlaboratory/Context_Engineering_LLM/cards/range\",  # scorad.json, labs.json, anthro.json \n",
    "]\n",
    "# Optional: a field schema jsonl with one object per line\n",
    "FIELD_SCHEMA_JSONL = \"/home/rijul/Gitlaboratory/Context_Engineering_LLM/cards/field_cards.jsonl\"  # ok if missing\n",
    "\n",
    "VSTORE_DIR = \"/home/rijul/Academic/Atopic Eczema/rag_faiss\"\n",
    "EMBED_MODEL = \"text-embedding-3-large\"   # or your allowed embedding model\n",
    "MODEL_NAME  = \"gpt-5-2025-08-07\"\n",
    "\n",
    "USE_PAGE_RENDER = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9a27cae-4f12-4864-986e-19f1f473c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "Intish = Union[int, str, None]\n",
    "\n",
    "class MedicalDataExtraction(BaseModel):\n",
    "    Duration: Optional[str] = None\n",
    "    Site_of_Onset: Optional[str] = None\n",
    "    Mode_of_Spread: Optional[str] = None\n",
    "    Symptoms: Optional[str] = None\n",
    "    Treatment_History: Optional[str] = None\n",
    "    Personal_History: Optional[str] = None\n",
    "    Birth_Preterm_Postterm: Optional[str] = None\n",
    "    Birth_Weight: Optional[str] = None\n",
    "    Mile_Stones: Optional[str] = None\n",
    "    Socio_Economic_Status: Optional[str] = None\n",
    "    Vaccination: Optional[str] = None\n",
    "    Number_of_Members_in_Household: Optional[int] = None\n",
    "    Family_History: Optional[str] = None\n",
    "    Family_Tree: Optional[str] = None\n",
    "    Past_History: Optional[str] = None\n",
    "    Similar_Ailments: Optional[str] = None\n",
    "    Pulse_Examination: Optional[str] = None\n",
    "    BP_Examination: Optional[str] = None\n",
    "    Pallor_Examination: Optional[str] = None\n",
    "    Cyanosis_Examination: Optional[str] = None\n",
    "    Jaundice_Examination: Optional[str] = None\n",
    "    Lymph_Nodes_Examination: Optional[str] = None\n",
    "    Weight_Examination: Optional[str] = None\n",
    "    Height_Examination: Optional[str] = None\n",
    "    Chest_Systemic_Examination: Optional[str] = None\n",
    "    CVS_Systemic_Examination: Optional[str] = None\n",
    "    Abdomen_Systemic_Examination: Optional[str] = None\n",
    "    CNS_Systemic_Examination: Optional[str] = None\n",
    "    Musculoskeletal_Systemic_Examination: Optional[str] = None\n",
    "    Face_Cutaneous_Examination: Optional[str] = None\n",
    "    Extremities_Cutaneous_Examination: Optional[str] = None\n",
    "    Palms_Cutaneous_Examination: Optional[str] = None\n",
    "    Flexures_Cutaneous_Examination: Optional[str] = None\n",
    "    Predominant_Site_Cutaneous_Examination: Optional[str] = None\n",
    "    Hairs_Cutaneous_Examination: Optional[str] = None\n",
    "    Muccous_Membrane_Cutaneous_Examination: Optional[str] = None\n",
    "    Oral_Cutaneous_Examination: Optional[str] = None\n",
    "    Genital_Cutaneous_Examination: Optional[str] = None\n",
    "    Scalp_Cutaneous_Examination: Optional[str] = None\n",
    "    Trunk_Cutaneous_Examination: Optional[str] = None\n",
    "    Soles_Cutaneous_Examination: Optional[str] = None\n",
    "    Nails_Cutaneous_Examination: Optional[str] = None\n",
    "    Nail_Folds_Cutaneous_Examination: Optional[str] = None\n",
    "    Nasal_Cutaneous_Examination: Optional[str] = None\n",
    "    Peri_anal_Cutaneous_Examination: Optional[str] = None\n",
    "    Extent_of_body_surface_area_involved: Optional[str] = None\n",
    "    Erythema_Intensity_Score: Intish = None\n",
    "    Edema_Intensity_Score: Intish = None\n",
    "    Excoriations_Intensity_Score: Intish = None\n",
    "    Oozing_Intensity_Score: Intish = None\n",
    "    Dryness_Intensity_Score: Intish = None\n",
    "    Lichenification_Intensity_Score: Intish = None\n",
    "    Total_Intensity_Score: Intish = None\n",
    "    Itchiness_Subjective_Score: Intish = None\n",
    "    Sleeplessness_Subjective_Score: Intish = None\n",
    "    Total_C_Subjective_Score: Intish = None\n",
    "    Final_SCORAD: Intish = None\n",
    "    Provisional_Diagnosis: Optional[str] = None\n",
    "    Hb_Investigations: Optional[str] = None\n",
    "    Na_Investigations: Optional[str] = None\n",
    "    ASL_Investigations: Optional[str] = None\n",
    "    TLC_Investigations: Optional[str] = None\n",
    "    DLC_Investigations: Optional[str] = None\n",
    "    ESR_Investigations: Optional[str] = None\n",
    "    Platelet_Investigations: Optional[str] = None\n",
    "    K_Investigations: Optional[str] = None\n",
    "    Urea_Investigations: Optional[str] = None\n",
    "    Creatinine_Investigations: Optional[str] = None\n",
    "    ALT_Investigations: Optional[str] = None\n",
    "    FBS_Investigations: Optional[str] = None\n",
    "    Serum_Billirubin_Investigations: Optional[str] = None\n",
    "    Serum_Proteins_Investigations: Optional[str] = None\n",
    "    Serum_IgE: Optional[str] = None\n",
    "    Ana_Investigations: Optional[str] = None\n",
    "    G6PD_Investigations: Optional[str] = None\n",
    "    Chest_Xray_Investigations: Optional[str] = None\n",
    "    Urine_RE_ME: Optional[str] = None\n",
    "    ECG_Investigations: Optional[str] = None\n",
    "    Echo_Cardiography_Investigations: Optional[str] = None\n",
    "    Biopsy_Histopathology: Optional[str] = None\n",
    "    Immunofluorescence: Optional[str] = None\n",
    "    Ultrasound: Optional[str] = None\n",
    "    MRI: Optional[str] = None\n",
    "    Other_Investigations: Optional[str] = None\n",
    "    Final_Diagnosis: Optional[str] = None\n",
    "    Treatment_Followup: Optional[str] = None\n",
    "    Followup_2: Optional[str] = None\n",
    "    Followup_3: Optional[str] = None\n",
    "    Followup_4: Optional[str] = None\n",
    "    Followup_5: Optional[str] = None\n",
    "\n",
    "    @field_validator(\n",
    "        \"Erythema_Intensity_Score\",\"Edema_Intensity_Score\",\"Excoriations_Intensity_Score\",\n",
    "        \"Oozing_Intensity_Score\",\"Dryness_Intensity_Score\",\"Lichenification_Intensity_Score\",\n",
    "        \"Total_Intensity_Score\",\"Itchiness_Subjective_Score\",\"Sleeplessness_Subjective_Score\",\n",
    "        \"Total_C_Subjective_Score\",\"Final_SCORAD\", mode=\"before\"\n",
    "    )\n",
    "    def coerce_intish(cls, v):\n",
    "        if v is None:\n",
    "            return None\n",
    "        if isinstance(v, int):\n",
    "            return v\n",
    "        s = str(v).strip()\n",
    "        for sep in [\"/\", \"-\", \"–\", \"—\", \"to\"]:\n",
    "            if sep in s:\n",
    "                s = s.split(sep)[0].strip()\n",
    "        try:\n",
    "            return int(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @field_validator(\n",
    "        \"Birth_Preterm_Postterm\", mode=\"before\"\n",
    "    )\n",
    "    def normalize_birth_status(cls, v):\n",
    "        if v is None:\n",
    "            return None\n",
    "        s = str(v).strip().lower()\n",
    "\n",
    "        # coarse cleanup\n",
    "        s = s.replace(\"-\", \" \").replace(\"_\", \" \").replace(\".\", \" \").replace(\"post term\", \"postterm\").replace(\"pre term\",\"preterm\")\n",
    "\n",
    "        # synonyms\n",
    "        synonyms = {\n",
    "            \"pt\": \"preterm\", \"pre\": \"preterm\", \"preterm\": \"preterm\",\n",
    "            \"term\": \"term\", \"full term\": \"term\", \"fullterm\": \"term\",\n",
    "            \"post\": \"postterm\", \"postterm\": \"postterm\", \"post term\": \"postterm\", \"post-term\": \"postterm\"\n",
    "        }\n",
    "        for k, val in synonyms.items():\n",
    "            if s == k or k in s.split():\n",
    "                s = val\n",
    "                break\n",
    "\n",
    "        # hard map to final labels\n",
    "        if \"preterm\" in s:\n",
    "            return \"Preterm\"\n",
    "        if s == \"term\":\n",
    "            return \"Term\"\n",
    "        if \"postterm\" in s:\n",
    "            return \"Postterm\"\n",
    "\n",
    "        # otherwise keep original (or return Not specified at your preference)\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85af9400-ffd8-4d3f-af44-a06d13b1f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image: Image.Image) -> str:\n",
    "    buf = io.BytesIO()\n",
    "    image.save(buf, format=\"PNG\")\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def render_pages_to_images(pdf_path: str, dpi: int = 240):\n",
    "    imgs = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for p in doc:\n",
    "            pix = p.get_pixmap(dpi=dpi)\n",
    "            imgs.append(Image.open(io.BytesIO(pix.tobytes(\"png\"))).convert(\"RGB\"))\n",
    "    return imgs\n",
    "\n",
    "def extract_images_from_pdf(pdf_path: str):\n",
    "    images = []\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            img_list = page.get_images(full=True)\n",
    "            for _, img in enumerate(img_list):\n",
    "                xref = img[0]\n",
    "                base_image = pdf_document.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                im = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "                images.append(im)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88904b5e-b8d7-4002-b412-5c713af350b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG vector store ready.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_json_file(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def flatten_json_to_text(data, card_id: str):\n",
    "    lines = [f\"[CARD_ID]: {card_id}\"]\n",
    "\n",
    "    def add_map(name, m):\n",
    "        if isinstance(m, dict) and m:\n",
    "            lines.append(f\"{name}:\")\n",
    "            for k, v in list(m.items())[:200]:\n",
    "                v_str = v if isinstance(v, str) else json.dumps(v, ensure_ascii=False)\n",
    "                lines.append(f\"  - {k} -> {v_str}\")\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key in [\n",
    "            \"description\", \"policy\", \"rules\", \"ranges\", \"synonyms\", \"map\",\n",
    "            \"frequency_map\", \"route_map\", \"dose_regex\", \"abbreviations\",\n",
    "            \"units\", \"canonicalization\", \"examples\", \"fields\"\n",
    "        ]:\n",
    "            if key in data:\n",
    "                val = data[key]\n",
    "                if isinstance(val, (str, int, float)):\n",
    "                    lines.append(f\"{key}: {val}\")\n",
    "                elif isinstance(val, dict):\n",
    "                    add_map(key, val)\n",
    "                elif isinstance(val, list):\n",
    "                    lines.append(f\"{key}:\")\n",
    "                    for item in val[:200]:\n",
    "                        if isinstance(item, dict):\n",
    "                            pair = \", \".join(f\"{k}={item[k]}\" for k in list(item.keys())[:4])\n",
    "                            lines.append(f\"  - {pair}\")\n",
    "                        else:\n",
    "                            lines.append(f\"  - {item}\")\n",
    "        for k, v in data.items():\n",
    "            if isinstance(v, (str, int, float)) and k not in {\"description\",\"policy\",\"rules\"}:\n",
    "                lines.append(f\"{k}: {v}\")\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        for i, item in enumerate(data[:300]):\n",
    "            if isinstance(item, dict):\n",
    "                pair = \", \".join(f\"{k}={item[k]}\" for k in list(item.keys())[:4])\n",
    "                lines.append(f\"- {pair}\")\n",
    "            else:\n",
    "                lines.append(f\"- {str(item)}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def iter_card_docs():\n",
    "    paths = []\n",
    "    for d in CARDS_DIRS:\n",
    "        if os.path.isdir(d):\n",
    "            paths.extend(glob.glob(os.path.join(d, \"**\", \"*.json\"), recursive=True))\n",
    "    for p in paths:\n",
    "        base = os.path.basename(p)\n",
    "        folder = os.path.basename(os.path.dirname(p))\n",
    "        card_id = f\"{folder}/{base}\"\n",
    "        try:\n",
    "            data = load_json_file(p)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {p}: {e}\")\n",
    "            continue\n",
    "        text = flatten_json_to_text(data, card_id)\n",
    "        yield Document(page_content=text, metadata={\"card_id\": card_id, \"source\": p})\n",
    "\n",
    "    if FIELD_SCHEMA_JSONL and os.path.exists(FIELD_SCHEMA_JSONL):\n",
    "        with open(FIELD_SCHEMA_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except Exception:\n",
    "                    obj = {\"raw\": line}\n",
    "                card_id = f\"schema/field_cards.jsonl#{i}\"\n",
    "                text = flatten_json_to_text(obj, card_id)\n",
    "                yield Document(page_content=text, metadata={\"card_id\": card_id, \"source\": FIELD_SCHEMA_JSONL})\n",
    "\n",
    "def build_or_load_vectorstore():\n",
    "    emb = OpenAIEmbeddings(model=EMBED_MODEL)  # picks up OPENAI_API_KEY from env\n",
    "    if os.path.exists(VSTORE_DIR) and os.path.isdir(VSTORE_DIR):\n",
    "        try:\n",
    "            return FAISS.load_local(VSTORE_DIR, emb, allow_dangerous_deserialization=True)\n",
    "        except Exception:\n",
    "            pass  # rebuild if corrupted\n",
    "\n",
    "    docs = list(iter_card_docs())\n",
    "    if not docs:\n",
    "        raise RuntimeError(\"No RAG cards found. Check CARDS_DIRS paths.\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    vstore = FAISS.from_documents(chunks, emb)\n",
    "    vstore.save_local(VSTORE_DIR)\n",
    "    return vstore\n",
    "\n",
    "# Build once\n",
    "vstore = build_or_load_vectorstore()\n",
    "retriever = vstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "print(\"RAG vector store ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f2bd32d-311a-4849-b381-764e3c757382",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_KEYS = list(MedicalDataExtraction.model_fields.keys())\n",
    "\n",
    "def schema_keys_block():\n",
    "    return (\n",
    "        \"SCHEMA KEYS (use EXACT names; fill string values or integers; \"\n",
    "        \"if absent/illegible use 'Not specified'):\\n- \" + \"\\n- \".join(SCHEMA_KEYS)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db370c70-e159-4f0c-a187-dd21a75ea4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def birth_policy_block():\n",
    "    return (\n",
    "        \"BIRTH FIELD POLICY:\\n\"\n",
    "        \"- The form shows: 'Birth: Pre term / Term / Post term'. Clinician usually CIRCLES one.\\n\"\n",
    "        \"- Choose exactly one of {Preterm, Term, Postterm} based on visual cues (circle, tick, underline, emphasis).\\n\"\n",
    "        \"- Normalize variants: 'pre term', 'pre-term', 'PT' -> Preterm; 'post term', 'post-term', 'post' -> Postterm.\\n\"\n",
    "        \"- If no clear mark, return 'Not specified'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7309f6db-0cb4-41dc-8fc6-01aa85db13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prefix(query: str, snippets):\n",
    "    joined = \"\\n\\n---\\n\\n\".join([d.page_content for d in snippets])\n",
    "    return (\n",
    "        \"USE THE FOLLOWING CARDS TO INTERPRET HANDWRITTEN CLINICAL NOTES. \"\n",
    "        \"Follow policies for abbreviations, dates, units, medication notation, SCORAD, labs, and ranges. \"\n",
    "        \"If a field is absent or illegible, set it to 'Not specified'. \"\n",
    "        \"Return ONLY the fields defined by the schema keys (exact names).\\n\\n\"\n",
    "        f\"{joined}\\n\\nEND OF CARDS.\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7e26e95-94e4-49f7-8f3d-0338f05079af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8), retry=retry_if_exception_type(Exception))\n",
    "def _call_model(message_content):\n",
    "    # 1) Preferred: structured parse path (Pydantic)\n",
    "    try:\n",
    "        resp = client.beta.chat.completions.parse(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": message_content}],\n",
    "            response_format=MedicalDataExtraction,\n",
    "            # NOTE: no temperature parameter (model enforces default)\n",
    "        )\n",
    "        return resp.choices[0].message.parsed\n",
    "    except Exception as e_parse:\n",
    "        print(\"[Parse path failed]\", e_parse)\n",
    "\n",
    "    # 2) Fallback: JSON mode\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message_content + [\n",
    "                    {\"type\": \"text\", \"text\": \"Return a single JSON object strictly matching the target schema.\"}\n",
    "                ]\n",
    "            }],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            # NOTE: no temperature parameter\n",
    "        )\n",
    "        data = _json_loads(resp.choices[0].message.content)\n",
    "        return MedicalDataExtraction(**data)\n",
    "    except Exception as e_json:\n",
    "        print(\"[JSON mode failed]\", e_json)\n",
    "\n",
    "    # 3) Last-resort: plain text response we parse as JSON\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message_content + [\n",
    "                {\"type\": \"text\", \"text\": \"Respond ONLY with a JSON object matching the target schema (no prose).\"}\n",
    "            ]\n",
    "        }],\n",
    "        # NOTE: no response_format, no temperature\n",
    "    )\n",
    "    txt = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # Try to extract a JSON object even if the model wrapped it in prose or backticks\n",
    "    try:\n",
    "        data = _json_loads(txt)\n",
    "    except Exception:\n",
    "        s, e = txt.find(\"{\"), txt.rfind(\"}\")\n",
    "        if s != -1 and e != -1 and e > s:\n",
    "            data = _json_loads(txt[s:e+1])\n",
    "        else:\n",
    "            raise RuntimeError(\"Could not parse model output as JSON.\")\n",
    "    return MedicalDataExtraction(**data)\n",
    "def extract_structured_data_from_images(images):\n",
    "    # RAG retrieval (static query; you can customize per PDF if desired)\n",
    "    query = (\n",
    "        \"Dermatology intake for atopic eczema; meds parsing, labs synonyms & units, \"\n",
    "        \"SCORAD rules, date normalization; strict schema extraction.\"\n",
    "    )\n",
    "    snippets = retriever.invoke(query)\n",
    "    rag_text = make_rag_prefix(query, snippets)\n",
    "    rag_cards = [d.metadata.get(\"card_id\") for d in snippets]\n",
    "\n",
    "    base64_images = [encode_image(im) for im in images]\n",
    "    message_content = [\n",
    "        {\"type\": \"text\", \"text\": schema_keys_block()},    # exact keys visible to the model\n",
    "        {\"type\": \"text\", \"text\": birth_policy_block()},   # <-- NEW: circled-choice instruction\n",
    "        {\"type\": \"text\", \"text\": rag_text},               # pinned + retrieved cards\n",
    "        {\"type\": \"text\", \"text\":\n",
    "            (\"Act as a clinician. Extract ONLY the schema keys (exact names). \"\n",
    "             \"If any field is absent or illegible, write 'Not specified'. \"\n",
    "             \"Do not invent values. Do not add extra keys.\")\n",
    "        }\n",
    "    ]\n",
    "# ... then append the images ...\n",
    "\n",
    "    for b64 in base64_images:\n",
    "        message_content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"}\n",
    "        })\n",
    "\n",
    "    parsed = _call_model(message_content)\n",
    "    return parsed, rag_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "668c85bb-06b3-489d-ba07-f24c77fd6a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Extracting: 1050.pdf\n",
      "[2] Extracting: 1619.pdf\n",
      "[3] Extracting: 2507.pdf\n",
      "[4] Extracting: 2664.pdf\n",
      "[5] Extracting: 2745.pdf\n",
      "[6] Extracting: 2799.pdf\n",
      "[7] Extracting: 2823.pdf\n",
      "[8] Extracting: 3226.pdf\n",
      "[9] Extracting: 3398.pdf\n",
      "[10] Extracting: 3460.pdf\n",
      "[11] Extracting: 3622.pdf\n",
      "[12] Extracting: 3731.pdf\n",
      "[13] Extracting: 3792.pdf\n",
      "[14] Extracting: 3850.pdf\n",
      "[15] Extracting: 3855.pdf\n",
      "[16] Extracting: 3867.pdf\n",
      "[17] Extracting: 3873.pdf\n",
      "[18] Extracting: 3886.pdf\n",
      "[19] Extracting: 3903.pdf\n",
      "[20] Extracting: 716.pdf\n",
      "Data extraction complete. Results saved to /home/rijul/Academic/Atopic Eczema/gpt5output_rag.xlsx.\n"
     ]
    }
   ],
   "source": [
    "structured_data = []\n",
    "i = 0\n",
    "\n",
    "for pdf_file in sorted(os.listdir(pdf_folder)):\n",
    "    if not pdf_file.lower().endswith(\".pdf\"):\n",
    "        continue\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    i += 1\n",
    "    print(f\"[{i}] Extracting: {pdf_file}\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        images = render_pages_to_images(pdf_path, dpi=240) if USE_PAGE_RENDER else extract_images_from_pdf(pdf_path)\n",
    "        parsed, rag_cards = extract_structured_data_from_images(images)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        try:\n",
    "            row = parsed.model_dump()\n",
    "        except Exception:\n",
    "            row = parsed.dict() if hasattr(parsed, \"dict\") else parsed.__dict__\n",
    "\n",
    "        row_out = {\n",
    "            \"Patient_ID\": os.path.splitext(pdf_file)[0],\n",
    "            \"Extraction_Time\": elapsed,\n",
    "            **row,\n",
    "            \"rag_cards\": \";\".join(rag_cards)\n",
    "        }\n",
    "        structured_data.append(row_out)\n",
    "    except Exception as e:\n",
    "        print(f\"   !! Failed on {pdf_file}: {e}\")\n",
    "        structured_data.append({\n",
    "            \"Patient_ID\": os.path.splitext(pdf_file)[0],\n",
    "            \"Extraction_Time\": time.time() - start_time,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "\n",
    "# Build the canonical column order from your Pydantic schema\n",
    "try:\n",
    "    SCHEMA_KEYS = list(MedicalDataExtraction.model_fields.keys())   # Pydantic v2\n",
    "except Exception:\n",
    "    SCHEMA_KEYS = [f for f in dir(MedicalDataExtraction) if not f.startswith(\"_\")]\n",
    "\n",
    "ordered_cols = (\n",
    "    [\"Patient_ID\", \"Extraction_Time\"]\n",
    "    + SCHEMA_KEYS\n",
    "    + ([\"rag_cards\"] if \"rag_cards\" in df.columns else [])\n",
    "    + ([\"error\"] if \"error\" in df.columns else [])\n",
    ")\n",
    "\n",
    "# Ensure all columns exist; add missing with NaN\n",
    "for c in ordered_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = pd.NA\n",
    "\n",
    "# Reindex to the exact schema order\n",
    "df = df.reindex(columns=ordered_cols)\n",
    "\n",
    "df = pd.DataFrame(structured_data)\n",
    "df.to_excel(output_excel, index=False)\n",
    "print(f\"Data extraction complete. Results saved to {output_excel}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
