{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e546450b",
   "metadata": {},
   "source": [
    "# Atopic Eczema VLM Extraction — RAG Pipeline (Gemma 3 27B IT)\n",
    "\n",
    "This notebook prototypes the full pipeline:\n",
    "1) Load RAG cards (fields, policies, abbrev, ranges, meds lexicon)  \n",
    "2) Candidate extraction (get page tokens from image)  \n",
    "3) RAG context assembly  \n",
    "4) Build prompts and call **google/gemma-3-27b-it** (Hugging Face)  \n",
    "5) Validate + compute confidences  \n",
    "6) Merge into a patient JSON\n",
    "\n",
    "> **Note:** You must accept the Gemma 3 license on Hugging Face and set `HF_TOKEN` in your environment to pull weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e74493",
   "metadata": {},
   "source": [
    "## 0. Environment & Installs\n",
    "\n",
    "- Make sure you have a suitable GPU (27B benefits from A100 / H100, bf16).\n",
    "- Install pinned `transformers` with Gemma 3 support and `accelerate`.\n",
    "- Login to Hugging Face or set an access token.\n",
    "- Accept the model license on its model card.\n",
    "\n",
    "**References:**\n",
    "- Hugging Face blog guide for Gemma 3 (inference & API usage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd897325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U 'transformers==4.49.0' accelerate torch torchvision pillow\\n# Optional: for better throughput / paged attention, also consider vLLM or TGI serving later\\n\\nimport os\\nos.environ.get(\"HF_TOKEN\")  # ensure your token is set, e.g., `export HF_TOKEN=hf_xxx` in your shell\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebfc716",
   "metadata": {},
   "source": [
    "## 1. Imports — Local Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45621fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from rag_store import RAGPaths, RAGStore, ContextAssembler, fields_for_section\n",
    "from candidate_extractor import CandidateExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7311987",
   "metadata": {},
   "source": [
    "## 2. Load RAG Cards\n",
    "Point to your cards directory (where we placed `field_cards.jsonl`, `policy/`, `abbr/`, `range/`, `lexicon/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "CARDS_DIR = Path(\"/home/rijul/Gitlaboratory/Context_Engineering_LLM/cards\")  # <-- update if different\n",
    "store = RAGStore(RAGPaths.from_base(CARDS_DIR)).load()\n",
    "\n",
    "print(\"Fields loaded:\", len(store.fields_by_name))\n",
    "print(\"Policies:\", list(store.policy.keys()))\n",
    "print(\"Abbr:\", list(store.abbr.keys()))\n",
    "print(\"Ranges:\", list(store.ranges.keys()))\n",
    "print(\"Lexicons:\", list(store.lexicons.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070099f",
   "metadata": {},
   "source": [
    "## 3. Candidate Extraction (VLM-assisted, no OCR)\n",
    "Given a form page image, ask the VLM to list headings/labels/short snippets likely to be variable names or medications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual page path(s)\n",
    "PAGE_IMAGE = \"/path/to/form_page1.png\"\n",
    "\n",
    "# Stub runner for demo. Replace with a real VLM call later.\n",
    "extractor = CandidateExtractor()\n",
    "page_tokens = extractor.extract_candidates(PAGE_IMAGE)\n",
    "print(\"Candidate tokens:\", page_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac41dd",
   "metadata": {},
   "source": [
    "## 4. Assemble RAG Context\n",
    "Select a section (e.g., `history`, `scorad`, `investigations`, `followups`) and build the compact context payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = ContextAssembler(store)\n",
    "target_fields = fields_for_section(\"history\")  # change to other sections as needed\n",
    "\n",
    "ctx = assembler.build_context(target_fields, page_tokens=page_tokens)\n",
    "chunks = assembler.to_prompt_chunks(ctx)\n",
    "\n",
    "for i, ch in enumerate(chunks, 1):\n",
    "    print(f\"=== Chunk {i} ===\\n{ch[:600]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555e8e3",
   "metadata": {},
   "source": [
    "## 5. Load Gemma 3 27B IT (Hugging Face)\n",
    "\n",
    "Use `transformers` **pipeline** for simple VLM calls (image + text → text).  \n",
    "Make sure you've accepted the model license on the model card: `google/gemma-3-27b-it`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41239d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Use bfloat16 on GPU if available\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# The \"image-text-to-text\" pipeline supports interleaved image/text messages per HF blog\n",
    "pipe = pipeline(\n",
    "    task=\"image-text-to-text\",\n",
    "    model=\"google/gemma-3-27b-it\",\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "\n",
    "# Minimal sanity query (uncomment when you have a real image path and HF auth)\n",
    "# messages = [{\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": [\n",
    "#         {\"type\": \"image\", \"image\": PAGE_IMAGE},\n",
    "#         {\"type\": \"text\", \"text\": \"List all headings visible on this page.\"}\n",
    "#     ]\n",
    "# }]\n",
    "# out = pipe(text=messages, max_new_tokens=200)\n",
    "# print(out[0][\"generated_text\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9beeb8c",
   "metadata": {},
   "source": [
    "## 6. Build the Field Extractor Prompt and Call the Model\n",
    "\n",
    "We send:\n",
    "- A **short system instruction**\n",
    "- The **RAG context chunks**\n",
    "- The **user instruction** describing what to extract\n",
    "- The **page image**\n",
    "\n",
    "We ask for **strict JSON** back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = (\n",
    "    \"You are a medical data extractor. \"\n",
    "    \"Use the provided field cards, policies, abbreviations, ranges, and meds lexicon to extract values. \"\n",
    "    \"If a value is missing or illegible, return null and set a low confidence. \"\n",
    "    \"Return only JSON.\"\n",
    ")\n",
    "\n",
    "USER_INSTR = (\n",
    "    \"Extract the requested fields from this page. \"\n",
    "    \"For each field, return {value, confidence (0..1), provenance: short description}. \"\n",
    "    \"Field set is in the context.\"\n",
    ")\n",
    "\n",
    "def build_messages(chunks, system_text, user_text, image_path):\n",
    "    # Build interleaved messages for the VLM pipeline\n",
    "    content = [{\"type\": \"text\", \"text\": system_text}]\n",
    "    # Append context chunks\n",
    "    for ch in chunks:\n",
    "        content.append({\"type\": \"text\", \"text\": ch})\n",
    "    # Append user instruction + image\n",
    "    content.append({\"type\": \"text\", \"text\": user_text})\n",
    "    content.append({\"type\": \"image\", \"image\": image_path})\n",
    "    return [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "messages = build_messages(chunks, SYSTEM, USER_INSTR, PAGE_IMAGE)\n",
    "\n",
    "# Example extraction call (uncomment to run with the actual model)\n",
    "# resp = pipe(text=messages, max_new_tokens=800)\n",
    "# raw = resp[0][\"generated_text\"][-1][\"content\"]\n",
    "# print(raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a9548",
   "metadata": {},
   "source": [
    "## 7. Validate & Normalize\n",
    "Apply ranges, unit/date normalization, and compute flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def validate_record(raw_json_text: str, store: RAGStore):\n",
    "    try:\n",
    "        data = json.loads(raw_json_text)\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": f\"JSON parse failed: {e}\", \"flags\": [], \"data\": None}\n",
    "\n",
    "    flags = []\n",
    "    # Simple examples\n",
    "    scorad_range = store.ranges.get(\"range/scorad:v1\", {}).get(\"ranges\", {})\n",
    "    if \"scorad_final\" in data:\n",
    "        v = data[\"scorad_final\"].get(\"value\")\n",
    "        if v is not None:\n",
    "            lo, hi = scorad_range.get(\"scorad_total\", [0, 103])\n",
    "            if not (lo <= float(v) <= hi):\n",
    "                flags.append({\"field\": \"scorad_final\", \"reason\": f\"Out of range [{lo},{hi}]\"})\n",
    "\n",
    "    return {\"ok\": True, \"flags\": flags, \"data\": data}\n",
    "\n",
    "# Example usage after a real model response:\n",
    "# result = validate_record(raw, store)\n",
    "# result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ae568",
   "metadata": {},
   "source": [
    "## 8. (Optional) Multi-page Merge\n",
    "If your PDF has multiple pages, run steps 3–7 per page and merge field-wise by confidence and page provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb21dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_records(records: list[dict]) -> dict:\n",
    "    merged = {}\n",
    "    for rec in records:\n",
    "        for k, v in rec.items():\n",
    "            if k not in merged:\n",
    "                merged[k] = v\n",
    "            else:\n",
    "                # keep the value with higher confidence\n",
    "                if v.get(\"confidence\", 0) > merged[k].get(\"confidence\", 0):\n",
    "                    merged[k] = v\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e9e8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Next\n",
    "- Replace the **stub candidate extractor** with a real Gemma call (Section 5) to list headings/blocks.  \n",
    "- Tune **Section selection** (batch fields in 10–15 chunks).  \n",
    "- Expand the **validator** with tighter clinical rules & ontology checks.  \n",
    "- Add a **resolver** step to re-query flagged fields with more focused crops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908372ed",
   "metadata": {},
   "source": [
    "## 9. PDF → Image Conversion (in-order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b472e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Requires: pip install pdf2image pillow  AND  poppler-utils (apt)\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Tuple\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def pdfs_to_images_in_series(pdf_dir: str, out_dir: str, dpi: int = 200) -> Iterator[Tuple[str, int, str]]:\n",
    "    \"\"\"\n",
    "    Convert all PDFs in pdf_dir to images in deterministic order.\n",
    "    Yields (pdf_file_path, page_index_1based, image_path).\n",
    "    \"\"\"\n",
    "    pdf_dir = Path(pdf_dir)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pdf_file in sorted(pdf_dir.glob(\"*.pdf\")):\n",
    "        patient_id = pdf_file.stem\n",
    "        pages = convert_from_path(str(pdf_file), dpi=dpi)\n",
    "        for i, page in enumerate(pages, start=1):\n",
    "            out_path = out_dir / f\"{patient_id}_page{i}.png\"\n",
    "            page.save(out_path, \"PNG\")\n",
    "            yield str(pdf_file), i, str(out_path)\n",
    "            \n",
    "print(\"PDF→Image helper ready. Set your input/output paths below.\")            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eaa65e",
   "metadata": {},
   "source": [
    "## 10. End-to-End Batch Loop (All PDFs → Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ff39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure your folders\n",
    "PDF_INPUT_DIR = \"/home/rijul/Academic/Atopic Eczema/cropped\"     # source PDFs\n",
    "IMG_OUTPUT_DIR = \"/home/rijul/Academic/Atopic Eczema/images\"     # where rendered PNGs will go\n",
    "\n",
    "# Choose which section to extract in this pass (you can run multiple passes for other sections)\n",
    "SECTION = \"history\"  # options: \"history\", \"scorad\", \"investigations\", \"followups\"\n",
    "\n",
    "# Instantiate helpers\n",
    "extractor = CandidateExtractor()\n",
    "assembler = ContextAssembler(store)\n",
    "target_fields = fields_for_section(SECTION)\n",
    "\n",
    "# Optional: real Gemma pipeline (uncomment if configured)\n",
    "# from transformers import pipeline\n",
    "# import torch, os\n",
    "# dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "# pipe = pipeline(\n",
    "#     task=\"image-text-to-text\",\n",
    "#     model=\"google/gemma-3-27b-it\",\n",
    "#     token=os.environ.get(\"HF_TOKEN\"),\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=dtype\n",
    "# )\n",
    "\n",
    "def build_messages(chunks, system_text, user_text, image_path):\n",
    "    content = [{\"type\": \"text\", \"text\": system_text}]\n",
    "    for ch in chunks: content.append({\"type\": \"text\", \"text\": ch})\n",
    "    content.append({\"type\": \"text\", \"text\": user_text})\n",
    "    content.append({\"type\": \"image\", \"image\": image_path})\n",
    "    return [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "SYSTEM = (\"You are a medical data extractor. Use the provided field cards, policies, abbreviations, ranges, \"\n",
    "          \"and meds lexicon to extract values. If a value is missing or illegible, return null and set a low confidence. \"\n",
    "          \"Return only JSON with keys matching canonical_name.\")\n",
    "\n",
    "USER_INSTR = (\"Extract the requested fields from this page. For each field, return \"\n",
    "              \"{value, confidence (0..1), provenance: short description}. Field set is in the context.\")\n",
    "\n",
    "results = []  # collect per-page outputs (replace with writing to disk if you prefer)\n",
    "\n",
    "for pdf_path, page_idx, img_path in pdfs_to_images_in_series(PDF_INPUT_DIR, IMG_OUTPUT_DIR, dpi=200):\n",
    "    # 1) Candidate tokens from the page\n",
    "    page_tokens = extractor.extract_candidates(img_path)\n",
    "\n",
    "    # 2) Assemble context for the chosen section\n",
    "    ctx = assembler.build_context(target_fields, page_tokens=page_tokens)\n",
    "    chunks = assembler.to_prompt_chunks(ctx)\n",
    "\n",
    "    # 3) Build messages for the VLM\n",
    "    messages = build_messages(chunks, SYSTEM, USER_INSTR, img_path)\n",
    "\n",
    "    # 4) Call the model (stub shown; uncomment for real call)\n",
    "    # resp = pipe(text=messages, max_new_tokens=800)\n",
    "    # raw = resp[0][\"generated_text\"][-1][\"content\"]\n",
    "    # For now, use a placeholder dict so the loop runs:\n",
    "    raw = '{\"duration\": {\"value\": \"6 months\", \"confidence\": 0.8, \"provenance\": \"upper right\"}, \"symptoms\": {\"value\": [\"itching\"], \"confidence\": 0.9, \"provenance\": \"middle\"}}'\n",
    "\n",
    "    # 5) Validate & store\n",
    "    vr = validate_record(raw, store)\n",
    "    results.append({\n",
    "        \"pdf\": pdf_path,\n",
    "        \"page\": page_idx,\n",
    "        \"image\": img_path,\n",
    "        \"raw\": raw,\n",
    "        \"validated\": vr\n",
    "    })\n",
    "\n",
    "# Example summary print\n",
    "print(f\"Processed pages: {len(results)}\")\n",
    "print(\"Sample record:\", results[0][\"pdf\"], results[0][\"page\"], results[0][\"validated\"][\"ok\"] if results else \"N/A\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
